# speech-emotion-recognition-done using cnn-lstm networks  
Dataset used is 4 classes from the EMODB Berlin Database for emotions
step1:visualising an input audio sample which is done using input.py
step2:its spectrogram
step3: feature extraction using mfcc for each class:4 classes used here angry, happy, sad and neutral
step4:Execute the mainn.py program which calls in subprograms in ser to train samples using
Accuracy of about 86% is obtained
